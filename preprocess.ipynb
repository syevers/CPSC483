{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "except Exception:\n",
    "    yaml = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Constants -------------------\n",
    "NULL_TOKENS = {\"\", \" \", \"na\", \"n/a\", \"null\", \"none\", \"unknown\", \"nan\", \".\", \"-\", \"--\"}\n",
    "\n",
    "US_STATES = {\n",
    "    \"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\"HI\",\"IA\",\"ID\",\"IL\",\"IN\",\"KS\",\"KY\",\"LA\",\"MA\",\"MD\",\n",
    "    \"ME\",\"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\"ND\",\"NE\",\"NH\",\"NJ\",\"NM\",\"NV\",\"NY\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\n",
    "    \"TN\",\"TX\",\"UT\",\"VA\",\"VT\",\"WA\",\"WI\",\"WV\",\"WY\",\"PR\",\"GU\",\"VI\",\"AS\",\"MP\"\n",
    "}\n",
    "\n",
    "# CMS scope/severity A–L → weights\n",
    "SEVERITY_WEIGHTS = {\n",
    "    \"A\":1, \"B\":1, \"C\":1,\n",
    "    \"D\":2, \"E\":2, \"F\":2,\n",
    "    \"G\":4, \"H\":4, \"I\":4,\n",
    "    \"J\":8, \"K\":8, \"L\":8\n",
    "}\n",
    "\n",
    "LOW_CARD_DEFAULT = [\n",
    "    \"state\",\"ownership_type\",\"in_hospital\",\"sprinkler_status\",\n",
    "    \"resident_and_family_councils\",\"chain_owner\"\n",
    "]\n",
    "HIGH_CARD_DEFAULT = [\"county\",\"zip\"]\n",
    "TEXT_DEFAULT = [\"provider_name\",\"legal_business_name\",\"parent_organization_name\",\"address\",\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Helpers -------------------\n",
    "def snake_case(name: str) -> str:\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"[^\\w]+\", \"_\", name)\n",
    "    name = re.sub(r\"__+\", \"_\", name)\n",
    "    return name.lower().strip(\"_\")\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [snake_case(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def clean_text_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).map(lambda x: unicodedata.normalize(\"NFKD\", x))\n",
    "    s = s.str.replace(r\"[\\u200B-\\u200D\\uFEFF]\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    s = s.str.replace(r\"[“”]\", '\"', regex=True).str.replace(r\"[‘’]\", \"'\", regex=True)\n",
    "    s = s.replace(NULL_TOKENS, np.nan)\n",
    "    return s\n",
    "\n",
    "def normalize_cats(s: pd.Series, mapping: dict = None, upper=False, title=False) -> pd.Series:\n",
    "    out = clean_text_series(s)\n",
    "    if upper:\n",
    "        out = out.str.upper()\n",
    "    elif title:\n",
    "        out = out.str.title()\n",
    "    if mapping:\n",
    "        out = out.map(lambda x: mapping.get(x, x) if pd.notna(x) else x)\n",
    "    return out\n",
    "\n",
    "def bucket_rare_levels(s: pd.Series, min_count: int = 50, other_label=\"Other\") -> pd.Series:\n",
    "    counts = s.value_counts(dropna=True)\n",
    "    rare = counts[counts < min_count].index\n",
    "    return s.where(~s.isin(rare), other_label)\n",
    "\n",
    "def fix_state_codes(s: pd.Series) -> pd.Series:\n",
    "    s = normalize_cats(s, upper=True)\n",
    "    s = s.where(s.isin(US_STATES), np.nan)\n",
    "    return s\n",
    "\n",
    "def derive_ccn_strict(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Derive strict 6-digit CCN, preserving leading zeros.\"\"\"\n",
    "    CCN_CANDIDATES = [\n",
    "        \"cms_certification_number_(ccn)\",\"cms_certification_number\",\"provider_number\",\n",
    "        \"facility_ccn\",\"cms_ccn\",\"ccn\"\n",
    "    ]\n",
    "    df = df.copy()\n",
    "    src_col = None\n",
    "    for c in CCN_CANDIDATES:\n",
    "        if c in df.columns:\n",
    "            src_col = c\n",
    "            break\n",
    "    if src_col is None:\n",
    "        for c in df.columns:\n",
    "            if \"ccn\" in c:\n",
    "                src_col = c\n",
    "                break\n",
    "    if src_col is not None:\n",
    "        s = df[src_col].astype(str)\n",
    "        digits = s.str.extract(r\"(\\d+)\", expand=False)\n",
    "        norm = digits.fillna(\"\")\n",
    "        norm = norm.apply(lambda x: x[-6:] if len(x) >= 6 else x.zfill(6) if len(x) > 0 else \"\")\n",
    "        df[\"ccn\"] = norm.replace(\"\", pd.NA)\n",
    "    else:\n",
    "        if \"ccn\" not in df.columns:\n",
    "            df[\"ccn\"] = pd.NA\n",
    "    return df\n",
    "\n",
    "def parse_any_date(s: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"coerce\", utc=False, infer_datetime_format=True)\n",
    "    except Exception:\n",
    "        return pd.to_datetime(pd.Series([pd.NA]*len(s)), errors=\"coerce\")\n",
    "\n",
    "def read_csv_loose(p: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(p, dtype=str, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Domain loaders -------------------\n",
    "def load_provider_info(path: Path) -> pd.DataFrame:\n",
    "    df = read_csv_loose(path)\n",
    "    df = normalize_columns(df)\n",
    "    df = derive_ccn_strict(df)\n",
    "\n",
    "    # numeric coercions\n",
    "    numeric_like = [\n",
    "        \"number_of_certified_beds\",\"average_number_of_residents_per_day\",\n",
    "        \"overall_rating\",\"staffing_rating\",\"rn_staffing_rating\",\"qm_rating\"\n",
    "    ]\n",
    "    for col in df.columns:\n",
    "        if any(x in col for x in numeric_like):\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col].str.replace(r\"[,$%]\", \"\", regex=True), errors=\"coerce\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # state & text\n",
    "    if \"state\" in df.columns:\n",
    "        df[\"state\"] = fix_state_codes(df[\"state\"])\n",
    "    for c in [\"provider_name\",\"legal_business_name\",\"parent_organization_name\",\"address\",\"city\",\n",
    "              \"ownership_type\",\"chain_owner\",\"in_hospital\",\"sprinkler_status\",\"resident_and_family_councils\",\n",
    "              \"county\",\"zip\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = clean_text_series(df[c])\n",
    "\n",
    "    # enforce one row per CCN in roster\n",
    "    df = df.drop_duplicates(subset=[\"ccn\"], keep=\"first\")\n",
    "    return df\n",
    "\n",
    "def load_penalties(path: Path, months_window: int = 36) -> pd.DataFrame:\n",
    "    df = read_csv_loose(path)\n",
    "    df = normalize_columns(df)\n",
    "    df = derive_ccn_strict(df)\n",
    "\n",
    "    # date column\n",
    "    date_cols = [c for c in df.columns if any(t in c for t in [\"date\",\"effective\",\"from\",\"start\",\"issued\"])]\n",
    "    dcol = date_cols[0] if date_cols else None\n",
    "    if dcol:\n",
    "        df[dcol] = parse_any_date(df[dcol])\n",
    "\n",
    "    # amount column\n",
    "    amt_cols = [c for c in df.columns if any(t in c for t in [\"fine\",\"amount\",\"penalty\",\"cmp\"])]\n",
    "    ac = amt_cols[0] if amt_cols else None\n",
    "    if ac:\n",
    "        df[ac] = pd.to_numeric(df[ac].str.replace(r\"[,$%]\", \"\", regex=True), errors=\"coerce\")\n",
    "\n",
    "    # payment denial flag\n",
    "    pd_cols = [c for c in df.columns if \"denial\" in c or \"payment_denial\" in c]\n",
    "    pdf = pd_cols[0] if pd_cols else None\n",
    "    if pdf:\n",
    "        df[pdf] = df[pdf].astype(str).str.upper().isin([\"Y\",\"YES\",\"TRUE\",\"1\"]).astype(\"int8\")\n",
    "\n",
    "    # window filter\n",
    "    if dcol:\n",
    "        cutoff = pd.Timestamp(\"today\").normalize() - pd.Timedelta(days=30*months_window)\n",
    "        df = df[df[dcol].isna() | (df[dcol] >= cutoff)]\n",
    "\n",
    "    gp = df.groupby(\"ccn\")\n",
    "    out = pd.DataFrame({\n",
    "        \"ccn\": gp.size().index,\n",
    "        \"penalty_events_36mo\": gp.size().values\n",
    "    })\n",
    "    out[\"total_fines_usd_36mo\"] = gp[ac].sum().values if ac else 0.0\n",
    "    out[\"had_any_payment_denial\"] = (gp[pdf].max()>0).astype(\"int8\").values if pdf else 0\n",
    "    return out\n",
    "\n",
    "def load_survey_dates(path: Path) -> pd.DataFrame:\n",
    "    df = read_csv_loose(path)\n",
    "    df = normalize_columns(df)\n",
    "    df = derive_ccn_strict(df)\n",
    "\n",
    "    # candidate date columns\n",
    "    candidates = [c for c in df.columns if (\"health\" in c and \"survey\" in c and \"date\" in c) or c.endswith(\"health_survey_date\")]\n",
    "    if not candidates:\n",
    "        candidates = [c for c in df.columns if (\"health\" in c and \"date\" in c)]\n",
    "\n",
    "    # cast to datetime\n",
    "    for col in df.columns:\n",
    "        if any(tok in col for tok in [\"date\",\"survey\",\"start\",\"end\"]):\n",
    "            dt = parse_any_date(df[col])\n",
    "            if dt.notna().any():\n",
    "                df[col] = dt\n",
    "\n",
    "    out = df[[\"ccn\"] + candidates].copy() if candidates else df[[\"ccn\"]].copy()\n",
    "    if candidates:\n",
    "        out[\"last_health_survey_date\"] = out[candidates].max(axis=1)\n",
    "    else:\n",
    "        out[\"last_health_survey_date\"] = pd.NaT\n",
    "\n",
    "    today = pd.Timestamp(\"today\").normalize()\n",
    "    out[\"days_since_last_health_survey\"] = (today - out[\"last_health_survey_date\"]).dt.days\n",
    "    return out[[\"ccn\",\"last_health_survey_date\",\"days_since_last_health_survey\"]]\n",
    "\n",
    "def load_survey_summary(path: Path) -> pd.DataFrame:\n",
    "    df = read_csv_loose(path)\n",
    "    df = normalize_columns(df)\n",
    "    df = derive_ccn_strict(df)\n",
    "    for col in df.columns:\n",
    "        if any(tok in col for tok in [\"count\",\"complaint\",\"revisit\",\"deficien\",\"tag\"]):\n",
    "            df[col] = pd.to_numeric(df[col].str.replace(r\"[,$%]\", \"\", regex=True), errors=\"coerce\")\n",
    "    out = df.groupby(\"ccn\", as_index=False).sum(numeric_only=True)\n",
    "    return out\n",
    "\n",
    "def load_health_citations(path: Path, months_window: int = 36) -> pd.DataFrame:\n",
    "    df = read_csv_loose(path)\n",
    "    df = normalize_columns(df)\n",
    "    df = derive_ccn_strict(df)\n",
    "\n",
    "    # severity code column\n",
    "    sev_col = None\n",
    "    for c in df.columns:\n",
    "        if \"scope\" in c and \"severity\" in c:\n",
    "            sev_col = c; break\n",
    "    if sev_col is None:\n",
    "        for c in df.columns:\n",
    "            if re.fullmatch(r\"[a-z_]*severity[a-z_]*\", c or \"\"):\n",
    "                sev_col = c; break\n",
    "\n",
    "    # date column\n",
    "    date_cols = [c for c in df.columns if any(tok in c for tok in [\"survey\",\"date\",\"visit\"])]\n",
    "    dcol = date_cols[0] if date_cols else None\n",
    "    if dcol:\n",
    "        df[dcol] = parse_any_date(df[dcol])\n",
    "        cutoff = pd.Timestamp(\"today\").normalize() - pd.Timedelta(days=30*months_window)\n",
    "        df = df[df[dcol].isna() | (df[dcol] >= cutoff)]\n",
    "\n",
    "    # map severity A–L to weights\n",
    "    if sev_col and sev_col in df.columns:\n",
    "        code = df[sev_col].astype(str).str.upper().str.extract(r\"([A-L])\", expand=False)\n",
    "        df[\"severity_weight\"] = code.map(SEVERITY_WEIGHTS).fillna(0).astype(float)\n",
    "    else:\n",
    "        df[\"severity_weight\"] = 0.0\n",
    "\n",
    "    out = df.groupby(\"ccn\", as_index=False)[\"severity_weight\"].sum()\n",
    "    out = out.rename(columns={\"severity_weight\":\"severity_weight_sum_36mo\"})\n",
    "    return out\n",
    "\n",
    "def load_vbp(path: Path) -> pd.DataFrame:\n",
    "    df = read_csv_loose(path)\n",
    "    df = normalize_columns(df)\n",
    "    df = derive_ccn_strict(df)\n",
    "    cand = [c for c in df.columns if \"multiplier\" in c or \"incentive\" in c]\n",
    "    if cand:\n",
    "        col = cand[0]\n",
    "        df[col] = pd.to_numeric(df[col].str.replace(r\"[,$%]\", \"\", regex=True), errors=\"coerce\")\n",
    "        return df[[\"ccn\", col]].rename(columns={col:\"vbp_incentive_multiplier\"}).drop_duplicates(\"ccn\")\n",
    "    return pd.DataFrame({\"ccn\":[], \"vbp_incentive_multiplier\":[]})\n",
    "\n",
    "def load_state_us_benchmarks(path: Path) -> pd.DataFrame:\n",
    "    df = read_csv_loose(path)\n",
    "    return normalize_columns(df)\n",
    "\n",
    "def load_qm_long(path: Path, source_label: str) -> pd.DataFrame:\n",
    "    \"\"\"Expect columns like: ccn, measure_id/name, value/rate/score, period_start, period_end (flexible).\"\"\"\n",
    "    df = read_csv_loose(path)\n",
    "    df = normalize_columns(df)\n",
    "    df = derive_ccn_strict(df)\n",
    "\n",
    "    # measure id\n",
    "    mid = \"measure_id\" if \"measure_id\" in df.columns else None\n",
    "    if mid is None:\n",
    "        for c in df.columns:\n",
    "            if \"measure\" in c and \"id\" in c:\n",
    "                mid = c; break\n",
    "\n",
    "    # value\n",
    "    val = None\n",
    "    for cand in [\"value\",\"rate\",\"score\",\"pct\",\"percent\",\"measure_value\"]:\n",
    "        if cand in df.columns:\n",
    "            val = cand; break\n",
    "\n",
    "    # period start / end\n",
    "    ps = None; pe = None\n",
    "    for c in df.columns:\n",
    "        if (\"period\" in c or \"start\" in c) and \"date\" in c:\n",
    "            ps = c; break\n",
    "    for c in df.columns:\n",
    "        if (\"period\" in c or \"end\" in c) and \"date\" in c:\n",
    "            pe = c; break\n",
    "    if ps is None:\n",
    "        for c in df.columns:\n",
    "            if c.endswith(\"start_date\") or c.endswith(\"period_start\"):\n",
    "                ps = c; break\n",
    "    if pe is None:\n",
    "        for c in df.columns:\n",
    "            if c.endswith(\"end_date\") or c.endswith(\"period_end\"):\n",
    "                pe = c; break\n",
    "\n",
    "    if ps: df[ps] = parse_any_date(df[ps])\n",
    "    if pe: df[pe] = parse_any_date(df[pe])\n",
    "    if val: df[val] = pd.to_numeric(df[val].str.replace(r\"[,$%]\", \"\", regex=True), errors=\"coerce\")\n",
    "\n",
    "    keep = [\"ccn\"]\n",
    "    if mid: keep.append(mid)\n",
    "    if val: keep.append(val)\n",
    "    if ps: keep.append(ps)\n",
    "    if pe: keep.append(pe)\n",
    "    out = df[keep].copy()\n",
    "    out[\"source\"] = source_label\n",
    "\n",
    "    rename_map = {}\n",
    "    if mid: rename_map[mid] = \"measure_id\"\n",
    "    if val: rename_map[val] = \"value\"\n",
    "    if ps: rename_map[ps] = \"period_start\"\n",
    "    if pe: rename_map[pe] = \"period_end\"\n",
    "    out = out.rename(columns=rename_map)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Feature builders -------------------\n",
    "def compute_citation_rate_per_bed(citations_sum: pd.DataFrame, provider_info: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = citations_sum.merge(provider_info[[\"ccn\",\"number_of_certified_beds\"]], on=\"ccn\", how=\"left\")\n",
    "    df[\"number_of_certified_beds\"] = pd.to_numeric(df[\"number_of_certified_beds\"], errors=\"coerce\")\n",
    "    df[\"severity_rate_per_100_beds\"] = df[\"severity_weight_sum_36mo\"] / df[\"number_of_certified_beds\"].replace(0, np.nan) * 100.0\n",
    "    return df[[\"ccn\",\"severity_rate_per_100_beds\"]]\n",
    "\n",
    "def latest_qm_per_facility(qm_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"period_end\" in qm_long.columns:\n",
    "        qm_long = qm_long.sort_values([\"ccn\",\"measure_id\",\"period_end\"])\n",
    "        latest = qm_long.groupby([\"ccn\",\"measure_id\"], as_index=False).tail(1)\n",
    "    else:\n",
    "        latest = qm_long.drop_duplicates([\"ccn\",\"measure_id\"], keep=\"last\")\n",
    "    return latest\n",
    "\n",
    "def load_measure_direction(config_path: Path = None):\n",
    "    lower_is_better = set()\n",
    "    if config_path and config_path.exists() and yaml is not None:\n",
    "        try:\n",
    "            cfg = yaml.safe_load(config_path.read_text())\n",
    "            lower_is_better = set(cfg.get(\"lower_is_better\", []))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return lower_is_better\n",
    "\n",
    "def standardize_qm_by_state_us(qm_latest: pd.DataFrame, provider_info: pd.DataFrame, bench: pd.DataFrame = None):\n",
    "    df = qm_latest.merge(provider_info[[\"ccn\",\"state\"]], on=\"ccn\", how=\"left\")\n",
    "\n",
    "    if bench is not None and all(c in bench.columns for c in [\"state\",\"measure_id\"]):\n",
    "        # expected optional cols: state_mean,state_std,us_mean,us_std\n",
    "        df = df.merge(bench, on=[\"state\",\"measure_id\"], how=\"left\", suffixes=(\"\",\"_bench\"))\n",
    "        state_mean = pd.to_numeric(df.get(\"state_mean\"), errors=\"coerce\")\n",
    "        state_std  = pd.to_numeric(df.get(\"state_std\"), errors=\"coerce\")\n",
    "        df[\"qm_z_state\"] = (df[\"value\"] - state_mean) / state_std.replace(0,np.nan)\n",
    "        if \"us_mean\" in df.columns and \"us_std\" in df.columns:\n",
    "            us_mean = pd.to_numeric(df[\"us_mean\"], errors=\"coerce\")\n",
    "            us_std  = pd.to_numeric(df[\"us_std\"], errors=\"coerce\")\n",
    "            df[\"qm_z_us\"] = (df[\"value\"] - us_mean) / us_std.replace(0,np.nan)\n",
    "    else:\n",
    "        # compute state stats from data\n",
    "        g = df.groupby([\"state\",\"measure_id\"])[\"value\"]\n",
    "        stats = g.agg([\"mean\",\"std\"]).reset_index().rename(columns={\"mean\":\"state_mean\",\"std\":\"state_std\"})\n",
    "        df = df.merge(stats, on=[\"state\",\"measure_id\"], how=\"left\")\n",
    "        df[\"qm_z_state\"] = (df[\"value\"] - df[\"state_mean\"]) / df[\"state_std\"].replace(0,np.nan)\n",
    "        # compute US stats\n",
    "        gus = df.groupby([\"measure_id\"])[\"value\"].agg([\"mean\",\"std\"]).reset_index().rename(columns={\"mean\":\"us_mean\",\"std\":\"us_std\"})\n",
    "        df = df.merge(gus, on=[\"measure_id\"], how=\"left\")\n",
    "        df[\"qm_z_us\"] = (df[\"value\"] - df[\"us_mean\"]) / df[\"us_std\"].replace(0,np.nan)\n",
    "\n",
    "    fac = df.groupby(\"ccn\", as_index=False).agg(qm_domain_z_state=(\"qm_z_state\",\"mean\"),\n",
    "                                                qm_domain_z_us=(\"qm_z_us\",\"mean\"))\n",
    "    return fac\n",
    "\n",
    "def frequency_encode(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    freqs = df[col].value_counts()\n",
    "    return df[col].map(freqs).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Pipeline -------------------\n",
    "def run_pipeline(data_dir: Path, out_dir: Path, months_window: int = 36, min_count_rare=50, measure_dir_config: Path = None):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Inputs (adjust names if your files differ)\n",
    "    p_provider     = data_dir / \"NH_ProviderInfo_Sep2025.csv\"\n",
    "    p_penalties    = data_dir / \"NH_Penalties_Sep2025.csv\"\n",
    "    p_survey_dates = data_dir / \"NH_SurveyDates_Sep2025.csv\"\n",
    "    p_survey_sum   = data_dir / \"NH_SurveySummary_Sep2025.csv\"\n",
    "    p_health_cit   = data_dir / \"NH_HealthCitations_Sep2025.csv\"\n",
    "    p_vbp          = data_dir / \"FY_2025_SNF_VBP_Facility_Performance.csv\"\n",
    "    p_bench        = data_dir / \"NH_StateUSAverages_Sep2025.csv\"\n",
    "    p_qm_claims    = data_dir / \"NH_QualityMsr_Claims_Sep2025.csv\"\n",
    "    p_qm_mds       = data_dir / \"NH_QualityMsr_MDS_Sep2025.csv\"\n",
    "\n",
    "    provider  = load_provider_info(p_provider) if p_provider.exists() else pd.DataFrame()\n",
    "    penalties = load_penalties(p_penalties, months_window) if p_penalties.exists() else pd.DataFrame(columns=[\"ccn\",\"penalty_events_36mo\",\"total_fines_usd_36mo\",\"had_any_payment_denial\"])\n",
    "    surv_dates= load_survey_dates(p_survey_dates) if p_survey_dates.exists() else pd.DataFrame(columns=[\"ccn\",\"last_health_survey_date\",\"days_since_last_health_survey\"])\n",
    "    surv_sum  = load_survey_summary(p_survey_sum) if p_survey_sum.exists() else pd.DataFrame(columns=[\"ccn\"])\n",
    "    citations = load_health_citations(p_health_cit, months_window) if p_health_cit.exists() else pd.DataFrame(columns=[\"ccn\",\"severity_weight_sum_36mo\"])\n",
    "    vbp       = load_vbp(p_vbp) if p_vbp.exists() else pd.DataFrame(columns=[\"ccn\",\"vbp_incentive_multiplier\"])\n",
    "    bench     = load_state_us_benchmarks(p_bench) if p_bench.exists() else None\n",
    "\n",
    "    sev_rate = compute_citation_rate_per_bed(citations, provider) if not citations.empty and not provider.empty else pd.DataFrame(columns=[\"ccn\",\"severity_rate_per_100_beds\"])\n",
    "\n",
    "    # QMs\n",
    "    qm_claims = load_qm_long(p_qm_claims, \"claims\") if p_qm_claims.exists() else pd.DataFrame(columns=[\"ccn\",\"measure_id\",\"value\",\"period_start\",\"period_end\",\"source\"])\n",
    "    qm_mds    = load_qm_long(p_qm_mds, \"mds\") if p_qm_mds.exists() else pd.DataFrame(columns=[\"ccn\",\"measure_id\",\"value\",\"period_start\",\"period_end\",\"source\"])\n",
    "    qm_all    = pd.concat([qm_claims, qm_mds], ignore_index=True) if not (qm_claims.empty and qm_mds.empty) else pd.DataFrame(columns=[\"ccn\",\"measure_id\",\"value\",\"period_start\",\"period_end\",\"source\"])\n",
    "\n",
    "    if not qm_all.empty:\n",
    "        lower_is_better = load_measure_direction(measure_dir_config)\n",
    "        if lower_is_better:\n",
    "            mask = qm_all[\"measure_id\"].isin(lower_is_better)\n",
    "            qm_all.loc[mask, \"value\"] = -qm_all.loc[mask, \"value\"]  # flip so \"higher is better\"\n",
    "        qm_latest = latest_qm_per_facility(qm_all)\n",
    "        qm_fac    = standardize_qm_by_state_us(qm_latest, provider, bench)\n",
    "    else:\n",
    "        qm_fac = pd.DataFrame(columns=[\"ccn\",\"qm_domain_z_state\",\"qm_domain_z_us\"])\n",
    "\n",
    "    # Assemble Gold\n",
    "    base_cols = [\"ccn\",\"state\",\"number_of_certified_beds\",\"average_number_of_residents_per_day\",\n",
    "                 \"overall_rating\",\"staffing_rating\",\"rn_staffing_rating\",\"qm_rating\"]\n",
    "    base_cols = [c for c in base_cols if c in provider.columns] + [\"ccn\"]  # safe subset\n",
    "    base_cols = list(dict.fromkeys(base_cols))  # unique, keep order\n",
    "\n",
    "    gold = provider[base_cols].copy()\n",
    "    for c in [\"ownership_type\",\"chain_owner\",\"in_hospital\",\"sprinkler_status\",\n",
    "              \"resident_and_family_councils\",\"city\",\"county\",\"zip\"]:\n",
    "        if c in provider.columns and c not in gold.columns:\n",
    "            gold[c] = provider[c]\n",
    "\n",
    "    for part in [penalties, surv_dates, sev_rate, vbp, qm_fac]:\n",
    "        if not part.empty:\n",
    "            gold = gold.merge(part, on=\"ccn\", how=\"left\")\n",
    "\n",
    "    # Clean/normalize categoricals\n",
    "    for c in TEXT_DEFAULT:\n",
    "        if c in gold.columns:\n",
    "            gold[c] = clean_text_series(gold[c])\n",
    "\n",
    "    if \"state\" in gold.columns:\n",
    "        gold[\"state\"] = fix_state_codes(gold[\"state\"])\n",
    "    OWNERSHIP_MAP = {\n",
    "        \"Government - Federal\":\"Government\",\n",
    "        \"Government - State\":\"Government\",\n",
    "        \"Government - County\":\"Government\",\n",
    "        \"For Profit\":\"For-Profit\",\n",
    "        \"Non Profit\":\"Non-Profit\"\n",
    "    }\n",
    "    if \"ownership_type\" in gold.columns:\n",
    "        gold[\"ownership_type\"] = normalize_cats(gold[\"ownership_type\"], mapping=OWNERSHIP_MAP, title=True)\n",
    "    for c in LOW_CARD_DEFAULT:\n",
    "        if c in gold.columns and c not in [\"ownership_type\",\"state\"]:\n",
    "            gold[c] = normalize_cats(gold[c], title=True)\n",
    "    for c in HIGH_CARD_DEFAULT:\n",
    "        if c in gold.columns:\n",
    "            gold[c] = normalize_cats(gold[c])\n",
    "            gold[c] = bucket_rare_levels(gold[c], min_count=min_count_rare, other_label=\"Other\")\n",
    "\n",
    "    # Missingness + impute numerics (by-state if available)\n",
    "    numeric_cols = [c for c in gold.columns if any(tok in c for tok in\n",
    "                     [\"number_of_certified_beds\",\"average_number_of_residents_per_day\",\n",
    "                      \"total_fines_usd\",\"penalty_events\",\"vbp_incentive_multiplier\",\n",
    "                      \"qm_domain_z\",\"severity_rate\",\"days_since_last_health_survey\"])]\n",
    "\n",
    "    for c in numeric_cols:\n",
    "        gold[f\"{c}_was_missing\"] = gold[c].isna().astype(\"int8\")\n",
    "\n",
    "    if \"state\" in gold.columns:\n",
    "        for c in numeric_cols:\n",
    "            gold[c] = gold.groupby(\"state\")[c].transform(lambda s: s.fillna(s.median()))\n",
    "            gold[c] = gold[c].fillna(gold[c].median())\n",
    "    else:\n",
    "        for c in numeric_cols:\n",
    "            gold[c] = gold[c].fillna(gold[c].median())\n",
    "\n",
    "    # Categorical missing\n",
    "    cat_cols = [c for c in gold.columns if gold[c].dtype == \"object\" and c not in [\"ccn\"]]\n",
    "    for c in cat_cols:\n",
    "        gold[c] = gold[c].fillna(\"Unknown\")\n",
    "\n",
    "    # Frequency-encode high-card\n",
    "    for c in HIGH_CARD_DEFAULT:\n",
    "        if c in gold.columns:\n",
    "            gold[c + \"_freq\"] = frequency_encode(gold, c)\n",
    "\n",
    "    # Save\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not provider.empty:  provider.to_csv(out_dir / \"clean_provider_info.csv\", index=False)\n",
    "    if not penalties.empty: penalties.to_csv(out_dir / \"clean_penalties_36mo.csv\", index=False)\n",
    "    if not surv_dates.empty:surv_dates.to_csv(out_dir / \"clean_survey_dates.csv\", index=False)\n",
    "    if not surv_sum.empty:  surv_sum.to_csv(out_dir / \"clean_survey_summary.csv\", index=False)\n",
    "    if not citations.empty: citations.to_csv(out_dir / \"clean_health_citations_36mo.csv\", index=False)\n",
    "    if not vbp.empty:       vbp.to_csv(out_dir / \"clean_vbp.csv\", index=False)\n",
    "    if not qm_all.empty:    qm_all.to_csv(out_dir / \"clean_qm_long.csv\", index=False)\n",
    "    if not qm_fac.empty:    qm_fac.to_csv(out_dir / \"clean_qm_facility_zscores.csv\", index=False)\n",
    "\n",
    "    gold = gold.drop_duplicates(subset=[\"ccn\"], keep=\"first\")\n",
    "    gold.to_csv(out_dir / \"NH_Gold_Feature_Table.csv\", index=False)\n",
    "\n",
    "    manifest = pd.DataFrame({\"feature\": gold.columns})\n",
    "    manifest.to_csv(out_dir / \"NH_Gold_Feature_Manifest.csv\", index=False)\n",
    "\n",
    "    # Simple report\n",
    "    report_lines = []\n",
    "    report_lines.append(\"NH Preprocessing Report\")\n",
    "    report_lines.append(f\"Facilities in provider roster: {len(provider)}\")\n",
    "    report_lines.append(f\"Gold rows (unique CCN): {gold['ccn'].nunique()}\")\n",
    "    report_lines.append(\"Key derived: severity_rate_per_100_beds, penalties_36mo, days_since_last_health_survey, qm_domain_z_state/us, vbp_incentive_multiplier\")\n",
    "    (out_dir / \"NH_Preprocessing_Report.txt\").write_text(\"\\n\".join(report_lines), encoding=\"utf-8\")\n",
    "\n",
    "    return gold\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--data-dir\", type=str, required=True, help=\"Directory with input CSVs\")\n",
    "    ap.add_argument(\"--out-dir\", type=str, required=True, help=\"Directory to write outputs\")\n",
    "    ap.add_argument(\"--months-window\", type=int, default=36, help=\"Window in months for penalties/citations\")\n",
    "    ap.add_argument(\"--min-count-rare\", type=int, default=50, help=\"Rare-category cutoff for HIGH_CARD columns\")\n",
    "    ap.add_argument(\"--measure-direction-config\", type=str, default=\"\", help=\"YAML with list lower_is_better: [measure_id,...]\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    data_dir = Path(args.data_dir)\n",
    "    out_dir = Path(args.out_dir)\n",
    "    cfg = Path(args.measure_direction_config) if args.measure_direction_config else None\n",
    "\n",
    "    run_pipeline(data_dir, out_dir,\n",
    "                 months_window=args.months_window,\n",
    "                 min_count_rare=args.min_count_rare,\n",
    "                 measure_dir_config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------handling outliers-------------\n",
    "#def function to remove outliers\n",
    "def remove_outliers_iqr(df, cols, multiplier=1.5):\n",
    "    df = df.copy()\n",
    "    for col in cols:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - multiplier * IQR\n",
    "            upper = Q3 + multiplier * IQR\n",
    "            df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "    return df\n",
    "\n",
    "    # identify numeric and categorical columns\n",
    "    num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    cat_cols = data.select_dtypes(exclude=[np.number]).columns\n",
    "    # handle missing values\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    data[num_cols] = num_imputer.fit_transform(data[num_cols])\n",
    "    data[cat_cols] = cat_imputer.fit_transform(data[cat_cols])\n",
    "    # remove outliers\n",
    "    data = remove_outliers_iqr(data, num_cols)\n",
    "    # encode categorical variables\n",
    "    data = pd.get_dummies(data, drop_first=True)\n",
    "    # scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    data[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------look for correlations---------\n",
    "# correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# print correlation with the target\n",
    "target_corr = corr_matrix[\"overall_rating\"].sort_values(ascending=False)\n",
    "print(\"Correlation of features with overall_rating:\\n\", target_corr)\n",
    "\n",
    "# full correlation matrix\n",
    "print(\"\\nFull correlation matrix:\\n\", corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
